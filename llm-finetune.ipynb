{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers accelerate peft bitsandbytes datasets rouge-score huggingface_hub matplotlib textblob\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')  \n",
    "\n",
    "from textblob import download_corpora\n",
    "download_corpora.download_all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb4aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Core Python ===\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Data Handling ===\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === NLP / Metrics ===\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from textblob import TextBlob\n",
    "\n",
    "# === HuggingFace Datasets ===\n",
    "from datasets import load_dataset\n",
    "\n",
    "# === Transformers & Training ===\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "# === PEFT (Parameter-Efficient Fine-Tuning) ===\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PromptTuningConfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c07a4",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe154127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/support_data.csv\", dtype={\n",
    "    \"tweet_id\": str,\n",
    "    \"in_response_to_tweet_id\": str\n",
    "})\n",
    "\n",
    "df = df.dropna(subset=[\"text\", \"in_response_to_tweet_id\", \"tweet_id\"])\n",
    "\n",
    "df[\"tweet_id\"] = df[\"tweet_id\"].astype(str)\n",
    "df[\"in_response_to_tweet_id\"] = df[\"in_response_to_tweet_id\"].astype(str)\n",
    "\n",
    "msg_lookup = df.set_index(\"tweet_id\")[\"text\"].to_dict()\n",
    "inbound_lookup = df.set_index(\"tweet_id\")[\"inbound\"].to_dict()\n",
    "\n",
    "pairs = []\n",
    "for _, row in df.iterrows():\n",
    "    msg_id = row[\"tweet_id\"]\n",
    "    in_response_to = row[\"in_response_to_tweet_id\"]\n",
    "\n",
    "    if not in_response_to in msg_lookup:\n",
    "        continue\n",
    "\n",
    "    if row[\"inbound\"] == False and inbound_lookup.get(in_response_to) == True:\n",
    "        customer_msg = msg_lookup[in_response_to]\n",
    "        brand_reply = row[\"text\"]\n",
    "        pairs.append({\"input\": customer_msg, \"output\": brand_reply})\n",
    "\n",
    "print(f\"‚úÖ Collected {len(pairs)} input-output pairs\")\n",
    "\n",
    "\n",
    "df_clean = pd.DataFrame(pairs).sample(frac=1.0, random_state=42).iloc[:25000]\n",
    "\n",
    "train_df, temp_df = train_test_split(df_clean, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "train_df.to_json(\"data/train.json\", orient=\"records\", lines=True)\n",
    "val_df.to_json(\"data/val.json\", orient=\"records\", lines=True)\n",
    "test_df.to_json(\"data/test.json\", orient=\"records\", lines=True)\n",
    "\n",
    "print(f\"‚úÖ Saved: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf86cb73",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab997d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a11b1",
   "metadata": {},
   "source": [
    "# inference logic and engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a90ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keywords = [\n",
    "    \"cancel\", \"refund\", \"charge\", \"billing\", \"dispute\", \"issue\",\n",
    "    \"angry\", \"unacceptable\", \"lawsuit\", \"legal\", \"scam\", \"fraud\",\n",
    "    \"complaint\", \"escalate\", \"speak to manager\", \"terrible\", \"disappointed\"\n",
    "]\n",
    "\n",
    "def score_priority(user_input):\n",
    "    score = sum(word in user_input.lower() for word in keywords)\n",
    "    if score >= 3:\n",
    "        return \"HIGH\"\n",
    "    elif score == 2:\n",
    "        return \"MEDIUM\"\n",
    "    else:\n",
    "        return \"LOW\"\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    if polarity < -0.3:\n",
    "        return \"NEGATIVE\"\n",
    "    elif polarity > 0.3:\n",
    "        return \"POSITIVE\"\n",
    "    else:\n",
    "        return \"NEUTRAL\"\n",
    "\n",
    "def generate_response(user_input, model, tokenizer):\n",
    "    priority = score_priority(user_input)\n",
    "    sentiment = analyze_sentiment(user_input)\n",
    "\n",
    "    prompt = (\n",
    "        f\"[SYSTEM: This agent must reply in a calm, helpful tone.]\\n\"\n",
    "        f\"[PRIORITY: {priority}] [SENTIMENT: {sentiment}] \"\n",
    "        f\"Customer message: {user_input}\\nAgent response:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return response  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2435ca9",
   "metadata": {},
   "source": [
    "# training logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c67aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossTracker(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            self.logs.append(logs)\n",
    "\n",
    "qlora_callback_r8 = LossTracker()\n",
    "qlora_callback_r4 = LossTracker()\n",
    "prompt_callback_20 = LossTracker()\n",
    "prompt_callback_10 = LossTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9156335",
   "metadata": {},
   "source": [
    "# Fine tuning method 1: qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Shared: Preprocess and Dataset --\n",
    "train_data = load_dataset(\"json\", data_files=\"data/train.json\")['train']\n",
    "val_data = load_dataset(\"json\", data_files=\"data/val.json\")['train']\n",
    "\n",
    "def preprocess(example):\n",
    "    prompt = f\"Customer message: {example['input']}\\nAgent response: {example['output']}\"\n",
    "    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "train_data = train_data.map(preprocess)\n",
    "val_data = val_data.map(preprocess)\n",
    "\n",
    "# -- Training Args (shared) --\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/qlora/\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# -- Config 1: QLoRA with r=8 --\n",
    "qlora_callback_r8 = LossTracker()\n",
    "qlora_model_r8 = prepare_model_for_kbit_training(base_model)\n",
    "lora_config_r8 = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "qlora_model_r8 = get_peft_model(qlora_model_r8, lora_config_r8)\n",
    "\n",
    "trainer_r8 = Trainer(\n",
    "    model=qlora_model_r8,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[qlora_callback_r8]\n",
    ")\n",
    "trainer_r8.train()\n",
    "\n",
    "# -- Config 2: QLoRA with r=4 (ablation) --\n",
    "qlora_callback_r4 = LossTracker()\n",
    "qlora_model_r4 = prepare_model_for_kbit_training(base_model)\n",
    "lora_config_r4 = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "qlora_model_r4 = get_peft_model(qlora_model_r4, lora_config_r4)\n",
    "\n",
    "trainer_r4 = Trainer(\n",
    "    model=qlora_model_r4,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[qlora_callback_r4]\n",
    ")\n",
    "trainer_r4.train()\n",
    "\n",
    "# -- Plot both loss curves --\n",
    "plot_loss(qlora_callback_r8.logs, \"QLoRA (r=8)\")\n",
    "plot_loss(qlora_callback_r4.logs, \"QLoRA (r=4)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38754d07",
   "metadata": {},
   "source": [
    "# fine tuning method 2: prompt tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11947b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- Config 1: 20 virtual tokens --\n",
    "prompt_config_20 = PromptTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    prompt_tuning_init=\"TEXT\",\n",
    "    num_virtual_tokens=20,\n",
    "    tokenizer_name_or_path=model_name\n",
    ")\n",
    "model_prompt_20 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model_prompt_20 = get_peft_model(model_prompt_20, prompt_config_20)\n",
    "prompt_callback_20 = LossTracker()\n",
    "trainer_prompt_20 = Trainer(\n",
    "    model=model_prompt_20,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[prompt_callback_20]\n",
    ")\n",
    "trainer_prompt_20.train()\n",
    "\n",
    "# -- Config 2: 10 virtual tokens (ablation) --\n",
    "prompt_config_10 = PromptTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    prompt_tuning_init=\"TEXT\",\n",
    "    num_virtual_tokens=10,\n",
    "    tokenizer_name_or_path=model_name\n",
    ")\n",
    "model_prompt_10 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model_prompt_10 = get_peft_model(model_prompt_10, prompt_config_10)\n",
    "prompt_callback_10 = LossTracker()\n",
    "trainer_prompt_10 = Trainer(\n",
    "    model=model_prompt_10,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[prompt_callback_10]\n",
    ")\n",
    "trainer_prompt_10.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5fb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(logs, title):\n",
    "    steps = [x[\"step\"] for x in logs if \"loss\" in x]\n",
    "    losses = [x[\"loss\"] for x in logs if \"loss\" in x]\n",
    "    plt.plot(steps, losses, label=\"Train Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(qlora_callback_r8.logs, \"QLoRA (r=8) Training Loss\")\n",
    "plot_loss(qlora_callback_r4.logs, \"QLoRA (r=4) Training Loss\")\n",
    "plot_loss(prompt_callback_20.logs, \"Prompt Tuning (20 Tokens) Training Loss\")\n",
    "plot_loss(prompt_callback_10.logs, \"Prompt Tuning (10 Tokens) Training Loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a7e85",
   "metadata": {},
   "source": [
    "# Evaluation (pre and post fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, name):\n",
    "    test_data = load_dataset(\"json\", data_files=\"data/test.json\")['train']\n",
    "    total_bleu = 0\n",
    "    total_rouge = {\"rouge-1\": 0, \"rouge-2\": 0, \"rouge-l\": 0}\n",
    "    start = time.time()\n",
    "\n",
    "    for ex in test_data:\n",
    "        prompt = f\"Customer message: {ex['input']}\\nAgent response:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        output = model.generate(**inputs, max_new_tokens=100)\n",
    "        pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        total_bleu += sentence_bleu([ex['output'].split()], pred.split())\n",
    "        scores = rouge.get_scores(pred, ex['output'])[0]\n",
    "        for k in total_rouge:\n",
    "            total_rouge[k] += scores[k][\"f\"]\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    n = len(test_data)\n",
    "\n",
    "    metrics = {\n",
    "        \"BLEU\": total_bleu / n,\n",
    "        \"ROUGE-1\": total_rouge[\"rouge-1\"] / n,\n",
    "        \"ROUGE-2\": total_rouge[\"rouge-2\"] / n,\n",
    "        \"ROUGE-L\": total_rouge[\"rouge-l\"] / n,\n",
    "        \"Latency\": total_time / n\n",
    "    }\n",
    "\n",
    "    print(f\"--- {name} MODEL EVALUATION ---\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "results = {\n",
    "    \"Base\": evaluate_model(base_model, name=\"Base\"),\n",
    "    \"QLoRA-8\": evaluate_model(qlora_model_r8, name=\"QLoRA-Tuned (r=8)\"),\n",
    "    \"QLoRA-4\": evaluate_model(qlora_model_r4, name=\"QLoRA-Tuned (r=4)\"),\n",
    "    \"Prompt-20\": evaluate_model(model_prompt_20, name=\"Prompt-Tuned-20\"),\n",
    "    \"Prompt-10\": evaluate_model(model_prompt_10, name=\"Prompt-Tuned-10\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5896e",
   "metadata": {},
   "source": [
    "# aggregating metrics and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665b19a",
   "metadata": {},
   "source": [
    "## metric plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7fdb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results).T[[\"BLEU\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]]\n",
    "\n",
    "# Plot\n",
    "df.plot(kind=\"bar\", figsize=(10, 6), ylim=(0, 1))\n",
    "plt.title(\"LLM Fine-Tuning Performance Comparison\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133ee8dd",
   "metadata": {},
   "source": [
    "## sample output comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_comparisons(n):\n",
    "    test_data = load_dataset(\"json\", data_files=\"data/test.json\")['train']\n",
    "    for i in range(n):\n",
    "        ex = test_data[i]\n",
    "        prompt = f\"Customer message: {ex['input']}\\nAgent response:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "\n",
    "        def get_response(model):\n",
    "            output = model.generate(**inputs, max_new_tokens=100)\n",
    "            return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            \n",
    "        print(f\"\\nInput: {ex['input']}\")\n",
    "        print(f\"Target: {ex['output']}\")\n",
    "        print(f\"Base: {get_response(base_model)}\")\n",
    "        print(f\"QLoRA-8: {get_response(qlora_model_r8)}\")\n",
    "        print(f\"QLoRA-4: {get_response(qlora_model_r4)}\")\n",
    "        print(f\"Prompt-20: {get_response(model_prompt_20)}\")\n",
    "        print(f\"Prompt-10: {get_response(model_prompt_10)}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "show_comparisons(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f2d81",
   "metadata": {},
   "source": [
    "## latency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies = {name: round(m[\"Latency\"], 2) for name, m in results.items()}\n",
    "print(\"‚è±Ô∏è Inference Latency per Sample (seconds):\")\n",
    "for k, v in latencies.items():\n",
    "    print(f\"{k}: {v}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de43ae",
   "metadata": {},
   "source": [
    "## trainable parameter count comparisons (why qlora and prompt tuning are effecient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_params(model, label):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    pct = (trainable / total) * 100\n",
    "    print(f\"{label} - Trainable params: {trainable:,} / {total:,} ({pct:.4f}%)\")\n",
    "\n",
    "print_trainable_params(qlora_model_r8, \"QLoRA (r=8)\")\n",
    "print_trainable_params(qlora_model_r4, \"QLoRA (r=4)\")\n",
    "print_trainable_params(model_prompt_20, \"Prompt Tuning (20)\")\n",
    "print_trainable_params(model_prompt_10, \"Prompt Tuning (10)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c65bb",
   "metadata": {},
   "source": [
    "## response category breakdown (error/response breakdown based on input intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_input(text):\n",
    "    if any(k in text.lower() for k in [\"cancel\", \"angry\", \"bad\", \"refund\", \"issue\"]):\n",
    "        return \"complaint\"\n",
    "    elif \"?\" in text:\n",
    "        return \"question\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "test_data = load_dataset(\"json\", data_files=\"data/test.json\")[\"train\"]\n",
    "test_data = test_data.map(lambda x: {\"category\": categorize_input(x[\"input\"])})\n",
    "\n",
    "def evaluate_by_category(model, name):\n",
    "    rouge = Rouge()\n",
    "    scores = defaultdict(lambda: {\"bleu\": [], \"rouge-l\": []})\n",
    "    \n",
    "    for ex in test_data:\n",
    "        prompt = f\"Customer message: {ex['input']}\\nAgent response:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        output = model.generate(**inputs, max_new_tokens=100)\n",
    "        pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        scores[ex[\"category\"]][\"bleu\"].append(sentence_bleu([ex[\"output\"].split()], pred.split()))\n",
    "        rouge_score = rouge.get_scores(pred, ex[\"output\"])[0][\"rouge-l\"][\"f\"]\n",
    "        scores[ex[\"category\"]][\"rouge-l\"].append(rouge_score)\n",
    "    \n",
    "    print(f\"\\n{name} Evaluation by Category\")\n",
    "    for cat in scores:\n",
    "        b = sum(scores[cat][\"bleu\"]) / len(scores[cat][\"bleu\"])\n",
    "        r = sum(scores[cat][\"rouge-l\"]) / len(scores[cat][\"rouge-l\"])\n",
    "        print(f\"{cat}: BLEU={b:.4f}, ROUGE-L={r:.4f}\")\n",
    "\n",
    "evaluate_by_category(qlora_model_r8, \"QLoRA\")\n",
    "evaluate_by_category(qlora_model_r4, \"QLoRA\")\n",
    "evaluate_by_category(model_prompt_20, \"Prompt Tuning (20)\")\n",
    "evaluate_by_category(model_prompt_10, \"Prompt Tuning (10)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef26f1",
   "metadata": {},
   "source": [
    "## token level error breakdown (worst bleu and rogue l samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eddbf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_worst_samples(model, tokenizer, test_data, metric=\"bleu\", n=5):\n",
    "    rouge = Rouge()\n",
    "    scored_samples = []\n",
    "\n",
    "    for ex in test_data:\n",
    "        prompt = f\"Customer message: {ex['input']}\\nAgent response:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        output = model.generate(**inputs, max_new_tokens=100)\n",
    "        pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        if metric == \"bleu\":\n",
    "            score = sentence_bleu([ex[\"output\"].split()], pred.split())\n",
    "        elif metric == \"rouge\":\n",
    "            score = rouge.get_scores(pred, ex[\"output\"])[0][\"rouge-l\"][\"f\"]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported metric.\")\n",
    "\n",
    "        scored_samples.append((score, ex[\"input\"], ex[\"output\"], pred))\n",
    "\n",
    "    scored_samples.sort(key=lambda x: x[0])\n",
    "    return scored_samples[:n]\n",
    "\n",
    "# Load test data\n",
    "test_data = load_dataset(\"json\", data_files=\"data/test.json\")[\"train\"]\n",
    "\n",
    "# Worst samples by BLEU\n",
    "worst_bleu_qlora_r8 = get_worst_samples(qlora_model_r8, tokenizer, test_data, metric=\"bleu\", n=5)\n",
    "worst_bleu_qlora_r4 = get_worst_samples(qlora_model_r4, tokenizer, test_data, metric=\"bleu\", n=5)\n",
    "worst_bleu_prompt_20 = get_worst_samples(model_prompt_20, tokenizer, test_data, metric=\"bleu\", n=5)\n",
    "worst_bleu_prompt_10 = get_worst_samples(model_prompt_10, tokenizer, test_data, metric=\"bleu\", n=5)\n",
    "\n",
    "# Worst samples by ROUGE-L\n",
    "worst_rouge_qlora_r8 = get_worst_samples(qlora_model_r8, tokenizer, test_data, metric=\"rouge\", n=5)\n",
    "worst_rouge_qlora_r4 = get_worst_samples(qlora_model_r4, tokenizer, test_data, metric=\"rouge\", n=5)\n",
    "worst_rouge_prompt_20 = get_worst_samples(model_prompt_20, tokenizer, test_data, metric=\"rouge\", n=5)\n",
    "worst_rouge_prompt_10 = get_worst_samples(model_prompt_10, tokenizer, test_data, metric=\"rouge\", n=5)\n",
    "\n",
    "# Display results\n",
    "def print_worst_cases(samples, method, metric):\n",
    "    print(f\"\\n--- Worst {metric.upper()} Cases for {method} ---\")\n",
    "    for score, inp, target, pred in samples:\n",
    "        print(f\"üîπ Input: {inp}\")\n",
    "        print(f\"‚úÖ Target: {target}\")\n",
    "        print(f\"üß† Prediction: {pred}\")\n",
    "        print(f\"üìâ {metric.upper()} Score: {score:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Print BLEU worst cases\n",
    "print_worst_cases(worst_bleu_qlora_r8, \"QLoRA (r=8)\", \"bleu\")\n",
    "print_worst_cases(worst_bleu_qlora_r4, \"QLoRA (r=4)\", \"bleu\")\n",
    "print_worst_cases(worst_bleu_prompt_20, \"Prompt Tuning (20)\", \"bleu\")\n",
    "print_worst_cases(worst_bleu_prompt_10, \"Prompt Tuning (10)\", \"bleu\")\n",
    "\n",
    "# Print ROUGE worst cases\n",
    "print_worst_cases(worst_rouge_qlora_r8, \"QLoRA (r=8)\", \"rouge\")\n",
    "print_worst_cases(worst_rouge_qlora_r4, \"QLoRA (r=4)\", \"rouge\")\n",
    "print_worst_cases(worst_rouge_prompt_20, \"Prompt Tuning (20)\", \"rouge\")\n",
    "print_worst_cases(worst_rouge_prompt_10, \"Prompt Tuning (10)\", \"rouge\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
